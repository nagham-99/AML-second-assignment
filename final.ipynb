{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://nipg1.inf.elte.hu:8765/MOSI.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UOBuAaWo0PV",
        "outputId": "ee121ac8-2fe5-4257-9520-c1e3f442ad5b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-28 21:42:16--  http://nipg1.inf.elte.hu:8765/MOSI.zip\n",
            "Resolving nipg1.inf.elte.hu (nipg1.inf.elte.hu)... 157.181.160.161\n",
            "Connecting to nipg1.inf.elte.hu (nipg1.inf.elte.hu)|157.181.160.161|:8765... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 513767740 (490M) [application/zip]\n",
            "Saving to: ‘MOSI.zip’\n",
            "\n",
            "MOSI.zip            100%[===================>] 489.97M  23.2MB/s    in 40s     \n",
            "\n",
            "2022-01-28 21:42:57 (12.1 MB/s) - ‘MOSI.zip’ saved [513767740/513767740]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "metadata": {
        "id": "w1gOSmn5o6d9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/assignment2'"
      ],
      "metadata": {
        "id": "GxmDBiQFqJND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pd.read_csv(drive_path + \"x_train.csv\").drop(columns=['Unnamed: 0'])\n",
        "y_train = pd.read_csv(drive_path + \"y_train.csv\").drop(columns=['Unnamed: 0'])\n",
        "z_train = pd.read_csv(drive_path + \"z_train.csv\").drop(columns=['Unnamed: 0'])\n",
        "x_valid = pd.read_csv(drive_path + \"x_valid.csv\").drop(columns=['Unnamed: 0'])\n",
        "y_valid = pd.read_csv(drive_path + \"y_valid.csv\").drop(columns=['Unnamed: 0'])\n",
        "z_valid = pd.read_csv(drive_path + \"z_valid.csv\").drop(columns=['Unnamed: 0'])\n",
        "x_test = pd.read_csv(drive_path + \"x_test.csv\").drop(columns=['Unnamed: 0'])\n",
        "y_test = pd.read_csv(drive_path + \"y_test.csv\").drop(columns=['Unnamed: 0'])\n",
        "z_test = pd.read_csv(drive_path + \"z_test.csv\").drop(columns=['Unnamed: 0'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pAuOlfAMr6KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "qpm8taqrtOOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "lbls = ['negative', 'positive']\n",
        "\n",
        "sns.countplot(x=y_train.colimns[0], data=y_train.iloc[:], ax=ax[0]).set_title('training')\n",
        "ax[0].set_xticklabeles(lbls)\n",
        "\n",
        "sns.countplot(x=y_valid.colimns[0], data=y_valid.iloc[:], ax=ax[1]).set_title('validation')\n",
        "ax[1].set_xticklabeles(lbls)\n",
        "\n",
        "sns.countplot(x=y_test.colimns[0], data=y_test.iloc[:], ax=ax[2]).set_title('testing')\n",
        "ax[2].set_xticklabeles(lbls)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VhJ06vbntR4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= pd.contact([x_test, x_train, x_valid]).astype(float).values\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "df = pd.DataFrame(x_scaled)\n",
        "x_test = df[0:len(x_test)]\n",
        "x_train = df[len(x_test):len(x_test)+len(x_train)]\n",
        "x_valid = df[(len(x_test)+len(x_train)):(len(x_test)+len(x_train)+len(x_valid))]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5W1EI6QYveUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import categorical\n",
        "categories = ['0', '1']\n",
        "\n",
        "categorical_column_sizes = [len(y_test[column])for column in categories]\n",
        "categorical_embedding_sizes = [(col_size, min(50, (col_size+1//2)) for col_size in categorical_column_sizes)]\n",
        "\n",
        "y_0 = y_train['0'].values\n",
        "y_1 = y_train['1'].values\n",
        "y_train = np.stack([y_0, y_1], 1)\n",
        "y_train = torch.tensor(y_train, dtype= torch.int64)\n",
        "zain = torch.tensor(z_train['0'].values).flatten()\n",
        "x_train = np.stack([x_train[col].values for col in x_train.columns], 1)\n",
        "x_train = torch.tensor(x_train, dtype=torch.float)\n",
        "\n",
        "\n",
        "y_0 = y_test['0'].values\n",
        "y_1 = y_test['1'].values\n",
        "y_test = np.stack([y_0, y_1], 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "z_test = torch.tensor(z_test['0'].values).flatten()\n",
        "\n",
        "x_test = np.stack([x_test[col].values for col in x_test.columns], 1)\n",
        "x_test = torch.tensor(x_test, dtype = torch.float)\n"
      ],
      "metadata": {
        "id": "r0WEFJhuxQdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, cat_emb_size, cat_cols, output_size, layers, dropout):\n",
        "      super().__init__()\n",
        "      self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in cat_emb_size])\n",
        "      self.embeding_dropout = nn.Dropout(dropout)\n",
        "      self.batch_norm_num = nn.BatchNorm1d(cat_cols)\n",
        "\n",
        "      model_layers = []\n",
        "      num_categorical_cols = sum(inf for ni, nf in cat_emb_size)\n",
        "      input_size = num_categorical_cols + cat_cols\n",
        "\n",
        "      for i in layers:\n",
        "        model_layers.append(nn.Linear(input_siza, i))\n",
        "        model_layers.append(nn.ReLU(inplace = True))\n",
        "        model_layers.append(nn.BatchNorm1d(i))\n",
        "        model_layers.append(nn.Dropout(dropout))\n",
        "        input_size = i\n",
        "      model_layers.append(nn.Linear(layers[-1], output_size))\n",
        "\n",
        "      self.layers = nn.Sequential(*model_layers)\n",
        "  def forward(self, x_categorical, x_numerical):\n",
        "    embeddings = []\n",
        "    for i,e in enumerate(self.all_embeddings):\n",
        "      embeddings.append(e(x_categorical[:, i]))\n",
        "    x = torch.cat(embeddings, 1)\n",
        "    x = self.embedding_dropout(x)\n",
        "    x= numerical =self.batch_norm_num(x_numerical)\n",
        "    x= torch.cat([x, x_numerical], 1)\n",
        "    x=self.layers(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "N3zoPTLz4QcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(categorical_embedding_sizes, x_train.shape[0], 2, [48, 48, 1])\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
        "epochs = 20\n",
        "aggregated_losses = []\n",
        "aggregated_valid_losses=[]\n",
        "aggregated_acc=[]\n",
        "aggregated_valid_acc=[]\n",
        "min_loss = np.inf\n",
        "\n"
      ],
      "metadata": {
        "id": "Hlle73nMC171"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  Y = model(y_train, x_train)\n",
        "  numerical_pred = np.argmax(Y.detach().numpy(), axis=1)\n",
        "  correct=len(np.where(numerical_pred==z_train.numpy()[0]))\n",
        "  aggregated_acc.append(correct/len(z_train))\n",
        "  single_loss = loss_function(Y, z_train)\n",
        "  aggregated_losses.append(single_loss.detach().numpy())\n",
        "  Y = model(y_valid, x_valid)\n",
        "  numerical_pred = np.argmax(Y.detach().numpy(), axis=1)\n",
        "  correct = len(np.where(numerical_pred==z_valid.numpy())[0])\n",
        "  aggregated_valid_acc.append(correct/len(z_valid))\n",
        "  single_loss = loss_function(Y, z_valid)\n",
        "  loss = single_loss.detach().numpy()\n",
        "  aggregated_valid_losses.append(loss)\n",
        "  optimizer.zero_grad()\n",
        "  single_loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f'epoch: {i+1:3} loss: {single_loss.item():8.6f} acc: {aggregated_acc[-1]:8.6f} valid_acc: {aggregated_valid_acc[-1]:8.6f}')\n"
      ],
      "metadata": {
        "id": "xg8hqcPLD6RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), aggregated_losses)\n",
        "plt.plot(range(epochs), aggregated_valid_losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12iXB2i5IoQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), aggregated_acc)\n",
        "plt.plot(range(epochs), aggregated_valid_acc)\n",
        "plt.ylabel('the accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q0MtnZF7Jhxf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}